---
title: "Gender Attribute Pretest Analysis"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    theme: flatly
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
                      fig.width = 10, fig.height = 6)

# Load required libraries
library(tidyverse)
library(qualtRics)
library(kableExtra)
library(ggthemes)
library(broom)
library(effectsize)

# Set theme for all plots
theme_set(theme_economist())
```

# Data Collection

```{r api-setup}
# Set up Qualtrics API credentials (should be configured in .Renviron)
# qualtrics_api_credentials(api_key = "YOUR_API_KEY",
#                          base_url = "YOUR_BASE_URL",
#                          install = TRUE)

# Survey ID for gender pretest
survey_id <- "SV_2mky9SMX7AwaQya"
```

```{r fetch-data}
# Fetch survey data using Qualtrics API
survey_data <- fetch_survey(surveyID = survey_id,
                            force_request = TRUE,
                            verbose = FALSE)

# Display initial data dimensions
cat("Initial sample size:", nrow(survey_data), "\n")
cat("Number of variables:", ncol(survey_data), "\n")
```

# Data Cleaning

```{r data-cleaning}
# First, let's examine the raw data to understand how scales are coded
cat("=== EXAMINING RAW DATA STRUCTURE ===\n\n")

# Check raw values for the importance questions
raw_check <- survey_data %>%
  filter(Finished == TRUE) %>%
  select(importance_1, importance_2, importance_3, importance_4) %>%
  slice_head(n = 20)  # Look at first 20 responses

cat("Sample of raw importance values (first 20 responses):\n")
print(raw_check)

# Check unique values for each importance question
cat("\n\nUnique values in each importance question:\n")
cat("importance_1 (female):", unique(survey_data$importance_1), "\n")
cat("importance_2 (budget):", unique(survey_data$importance_2), "\n")
cat("importance_3 (recent):", unique(survey_data$importance_3), "\n")
cat("importance_4 (political):", unique(survey_data$importance_4), "\n\n")

# Function to clean scale values - handles Qualtrics text anchors properly
clean_scale <- function(x) {
  x <- as.character(x)

  # Handle specific Qualtrics text anchors found in data:
  # "Not at all important 1" -> 1
  # "Very important7" -> 7
  # Regular numbers stay as-is
  x <- case_when(
    grepl("Not at all important.*1", x) ~ "1",
    grepl("Very important.*7", x) ~ "7",
    TRUE ~ x
  )

  # Remove any remaining non-numeric characters
  x <- gsub("[^0-9]", "", x)
  return(as.numeric(x))
}

# Clean and prepare data
clean_data <- survey_data %>%
  # Filter for completed responses
  filter(Finished == TRUE) %>%
  # Filter for those who passed attention checks
  filter(!is.na(ec_1_1) & !is.na(ec_2)) %>%
  # Select relevant columns
  select(ResponseId, StartDate, EndDate, Duration = `Duration (in seconds)`,
         # Importance ratings
         importance_female = importance_1,  # Feature a female protagonist
         importance_budget = importance_2,  # Had a big budget
         importance_recent = importance_3,  # Released after 2010
         importance_political = importance_4, # Feature a political leader
         # Demographics
         gender, race,
         # Comments
         comments = end)

# Show raw values before cleaning
cat("=== VALUES BEFORE CLEANING ===\n")
temp_check <- clean_data %>%
  select(starts_with("importance_")) %>%
  slice_head(n = 10)
print(temp_check)

# Apply cleaning
clean_data <- clean_data %>%
  # Clean importance ratings (handle text anchors if present)
  mutate(across(starts_with("importance_"), clean_scale)) %>%
  # EXCLUDE participants who didn't answer ALL four importance questions
  filter(complete.cases(importance_female, importance_budget,
                        importance_recent, importance_political))

# Display data dimensions at each step
cat("\n=== SAMPLE SIZE TRACKING ===\n")
cat("Initial sample size:", nrow(survey_data), "\n")
cat("After filtering for completion and attention checks:",
    nrow(filter(survey_data, Finished == TRUE, !is.na(ec_1_1), !is.na(ec_2))), "\n")
cat("After excluding incomplete importance ratings:", nrow(clean_data), "\n\n")

# Check for missing values in importance ratings (should be 0 now)
missing_importance <- clean_data %>%
  select(starts_with("importance_")) %>%
  summarise_all(~sum(is.na(.)))

kable(missing_importance,
      caption = "Missing Values in Importance Ratings (After Filtering)") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

# Show distribution of values to confirm scale is 1-7
cat("\n=== FINAL SCALE DISTRIBUTION ===\n")
scale_check <- clean_data %>%
  select(starts_with("importance_")) %>%
  pivot_longer(everything(), names_to = "item", values_to = "value") %>%
  count(value) %>%
  arrange(value)

# Check if 1 and 7 are present
has_ones <- 1 %in% scale_check$value
has_sevens <- 7 %in% scale_check$value

if(!has_ones || !has_sevens) {
  cat("WARNING: Scale may not be properly coded!\n")
  cat("  - Contains 1s:", has_ones, "\n")
  cat("  - Contains 7s:", has_sevens, "\n")
  cat("  - Actual range:", min(scale_check$value), "-", max(scale_check$value), "\n\n")
}

kable(scale_check,
      col.names = c("Scale Value", "Frequency"),
      caption = "Distribution of Scale Values (Should be 1-7 Range)") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

# Additional diagnostic: show distribution by item
item_dist <- clean_data %>%
  select(starts_with("importance_")) %>%
  pivot_longer(everything(), names_to = "item", values_to = "value") %>%
  group_by(item, value) %>%
  count() %>%
  pivot_wider(names_from = value, values_from = n, values_fill = 0)

cat("\n=== DISTRIBUTION BY ITEM ===\n")
kable(item_dist,
      caption = "Response Distribution by Item") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

# Descriptive Statistics

## Sample Demographics

```{r demographics}
# Gender distribution
gender_dist <- clean_data %>%
  count(gender) %>%
  mutate(percentage = round(n / sum(n) * 100, 1))

# Race distribution
race_dist <- clean_data %>%
  count(race) %>%
  mutate(percentage = round(n / sum(n) * 100, 1))

# Display demographics tables
kable(gender_dist,
      col.names = c("Gender", "N", "%"),
      caption = "Gender Distribution") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

kable(race_dist,
      col.names = c("Race/Ethnicity", "N", "%"),
      caption = "Race/Ethnicity Distribution") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

## Importance Ratings Summary

```{r importance-summary}
# Calculate summary statistics for each attribute
importance_summary <- clean_data %>%
  select(starts_with("importance_")) %>%
  pivot_longer(everything(), names_to = "attribute", values_to = "rating") %>%
  mutate(attribute = case_when(
    attribute == "importance_female" ~ "Female Protagonist",
    attribute == "importance_budget" ~ "Big Budget (>$40M)",
    attribute == "importance_recent" ~ "Released After 2010",
    attribute == "importance_political" ~ "Political Leader"
  )) %>%
  group_by(attribute) %>%
  summarise(
    N = n(),
    Mean = round(mean(rating, na.rm = TRUE), 2),
    SD = round(sd(rating, na.rm = TRUE), 2),
    Median = median(rating, na.rm = TRUE),
    Min = min(rating, na.rm = TRUE),
    Max = max(rating, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(desc(Mean))

kable(importance_summary,
      caption = "Importance Ratings Summary Statistics") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

# Statistical Analysis

## OLS Regression Analysis

Testing differences in importance ratings across attributes using OLS regression with attribute dummy variables.

```{r regression-setup}
# Reshape data for regression
regression_data <- clean_data %>%
  select(ResponseId, starts_with("importance_")) %>%
  pivot_longer(-ResponseId, names_to = "attribute", values_to = "rating") %>%
  mutate(
    # Create dummy variables with Female Protagonist as reference category
    is_budget = ifelse(attribute == "importance_budget", 1, 0),
    is_recent = ifelse(attribute == "importance_recent", 1, 0),
    is_political = ifelse(attribute == "importance_political", 1, 0),
    attribute_label = factor(case_when(
      attribute == "importance_female" ~ "Female Protagonist (Reference)",
      attribute == "importance_budget" ~ "Big Budget",
      attribute == "importance_recent" ~ "Recent Release",
      attribute == "importance_political" ~ "Political Leader"
    ))
  )
```

## Main OLS Model

```{r ols-main}
# Run OLS regression with Female Protagonist as reference
ols_model <- lm(rating ~ is_budget + is_recent + is_political,
                data = regression_data)
summary(ols_model)
# Get tidy summary
ols_summary <- tidy(ols_model, conf.int = TRUE)

# Add interpretation column
ols_summary <- ols_summary %>%
  mutate(
    term = case_when(
      term == "(Intercept)" ~ "Female Protagonist (Baseline)",
      term == "is_budget" ~ "Big Budget vs. Female",
      term == "is_recent" ~ "Recent Release vs. Female",
      term == "is_political" ~ "Political Leader vs. Female"
    ),
    significant = ifelse(p.value < 0.05, "Yes", "No")
  )

# Display regression results
kable(ols_summary %>%
        select(Coefficient = term,
               Estimate = estimate,
               SE = std.error,
               `95% CI Lower` = conf.low,
               `95% CI Upper` = conf.high,
               `t-value` = statistic,
               `p-value` = p.value,
               Significant = significant),
      digits = 3,
      caption = "OLS Regression Results (Female Protagonist as Reference)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  row_spec(which(ols_summary$significant == "Yes"), bold = TRUE)

# Model statistics
cat("\nModel Statistics:\n")
cat("R-squared:", round(summary(ols_model)$r.squared, 3), "\n")
cat("Adjusted R-squared:", round(summary(ols_model)$adj.r.squared, 3), "\n")
cat("F-statistic:", round(summary(ols_model)$fstatistic[1], 2), "\n")
cat("Overall p-value:", format.pval(pf(summary(ols_model)$fstatistic[1],
                                       summary(ols_model)$fstatistic[2],
                                       summary(ols_model)$fstatistic[3],
                                       lower.tail = FALSE)), "\n")
```

## Pairwise T-Tests

```{r t-tests}
# Function to calculate Cohen's d for paired samples
cohens_d_paired <- function(x, y) {
  diff <- x - y
  sd_diff <- sd(diff, na.rm = TRUE)
  mean_diff <- mean(diff, na.rm = TRUE)

  # Cohen's d for paired samples
  d <- mean_diff / sd_diff

  # Classify magnitude
  magnitude <- case_when(
    abs(d) < 0.2 ~ "negligible",
    abs(d) < 0.5 ~ "small",
    abs(d) < 0.8 ~ "medium",
    TRUE ~ "large"
  )

  return(list(estimate = d, magnitude = magnitude))
}

# Create list to store results
t_test_results <- list()

# Define attribute pairs for comparison
comparisons <- list(
  c("importance_female", "importance_budget", "Female Protagonist", "Big Budget"),
  c("importance_female", "importance_recent", "Female Protagonist", "Recent Release"),
  c("importance_female", "importance_political", "Female Protagonist", "Political Leader"),
  c("importance_budget", "importance_recent", "Big Budget", "Recent Release"),
  c("importance_budget", "importance_political", "Big Budget", "Political Leader"),
  c("importance_recent", "importance_political", "Recent Release", "Political Leader")
)

# Perform t-tests for each comparison
for(comp in comparisons) {
  var1 <- comp[1]
  var2 <- comp[2]
  label1 <- comp[3]
  label2 <- comp[4]

  # Paired t-test (since same participants rate all attributes)
  t_result <- t.test(clean_data[[var1]], clean_data[[var2]], paired = TRUE)

  # Calculate Cohen's d for effect size
  cohen_d <- cohens_d_paired(clean_data[[var1]], clean_data[[var2]])

  # Store results
  t_test_results[[paste(label1, "vs", label2)]] <- list(
    comparison = paste(label1, "vs", label2),
    mean_diff = mean(clean_data[[var1]] - clean_data[[var2]], na.rm = TRUE),
    t_stat = t_result$statistic,
    df = t_result$parameter,
    p_value = t_result$p.value,
    ci_lower = t_result$conf.int[1],
    ci_upper = t_result$conf.int[2],
    cohens_d = cohen_d$estimate,
    d_magnitude = cohen_d$magnitude
  )
}

# Convert to data frame
t_test_df <- bind_rows(t_test_results) %>%
  mutate(
    # Apply multiple comparison corrections
    p_bonferroni = p.adjust(p_value, method = "bonferroni", n = 6),
    p_holm = p.adjust(p_value, method = "holm", n = 6),

    # Determine significance at different correction levels
    sig_uncorrected = ifelse(p_value < 0.05, "Yes", "No"),
    sig_bonferroni = ifelse(p_bonferroni < 0.05, "Yes", "No"),
    sig_holm = ifelse(p_holm < 0.05, "Yes", "No"),

    # Format for display
    mean_diff = round(mean_diff, 3),
    t_stat = round(t_stat, 2),
    cohens_d = round(cohens_d, 3),
    p_value = round(p_value, 4),
    p_bonferroni = round(p_bonferroni, 4),
    p_holm = round(p_holm, 4),
    ci_lower = round(ci_lower, 3),
    ci_upper = round(ci_upper, 3)
  )

# Display main results table
kable(t_test_df %>%
        select(Comparison = comparison,
               `Mean Diff` = mean_diff,
               `t` = t_stat,
               `df` = df,
               `p-value` = p_value,
               `p (Bonf.)` = p_bonferroni,
               `Cohen's d` = cohens_d,
               `Effect Size` = d_magnitude),
      caption = "Pairwise T-Tests Between Attributes") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  row_spec(which(t_test_df$sig_uncorrected == "Yes"), bold = TRUE) %>%
  footnote(general = "Bold rows indicate p < 0.05 (uncorrected). Bonferroni correction applied for 6 comparisons.")

# Summary of significant findings
cat("\n=== T-TEST SUMMARY ===\n")
cat("Total comparisons:", nrow(t_test_df), "\n")
cat("Significant (uncorrected):", sum(t_test_df$sig_uncorrected == "Yes"), "\n")
cat("Significant (Holm correction):", sum(t_test_df$sig_holm == "Yes"), "\n")
cat("Significant (Bonferroni):", sum(t_test_df$sig_bonferroni == "Yes"), "\n\n")

# Check if any comparison involving female protagonist is significant
female_comparisons <- t_test_df %>%
  filter(grepl("Female Protagonist", comparison))

if(any(female_comparisons$sig_bonferroni == "Yes")) {
  cat("WARNING: Significant differences found with Female Protagonist attribute\n")
  cat("even after Bonferroni correction:\n")
  sig_female <- female_comparisons %>%
    filter(sig_bonferroni == "Yes")
  for(i in 1:nrow(sig_female)) {
    cat("  -", sig_female$comparison[i], "(p =", sig_female$p_bonferroni[i], ")\n")
  }
} else {
  cat("✓ No significant differences with Female Protagonist attribute\n")
  cat("  after correcting for multiple comparisons\n")
}
```

## Effect Size Interpretation

```{r effect-sizes}
# Create effect size interpretation guide
effect_guide <- data.frame(
  `Cohen's d Range` = c("< 0.2", "0.2 - 0.5", "0.5 - 0.8", "> 0.8"),
  Interpretation = c("Negligible", "Small", "Medium", "Large"),
  `Practical Significance` = c(
    "No practical difference",
    "Minor difference, likely not meaningful",
    "Moderate difference, potentially meaningful",
    "Substantial difference, likely meaningful"
  ),
  stringsAsFactors = FALSE
)

kable(effect_guide,
      caption = "Cohen's d Effect Size Interpretation Guide") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

# Show distribution of effect sizes
cat("\n=== EFFECT SIZE DISTRIBUTION ===\n")
effect_summary <- t_test_df %>%
  group_by(d_magnitude) %>%
  summarise(
    Count = n(),
    `Mean |d|` = round(mean(abs(cohens_d)), 3),
    .groups = "drop"
  )

print(effect_summary)

# Identify largest effect sizes
cat("\nLargest effect sizes:\n")
largest_effects <- t_test_df %>%
  arrange(desc(abs(cohens_d))) %>%
  slice_head(n = 3)

for(i in 1:nrow(largest_effects)) {
  cat(i, ". ", largest_effects$comparison[i], ": d = ",
      largest_effects$cohens_d[i], " (", largest_effects$d_magnitude[i], ")\n", sep = "")
}
```

## Pairwise Comparisons

```{r pairwise-ols}
# Test all pairwise differences using different reference categories
comparisons <- list()

# Budget as reference
model_budget <- lm(rating ~ I(attribute == "importance_female") +
                            I(attribute == "importance_recent") +
                            I(attribute == "importance_political"),
                  data = regression_data)
budget_coef <- coef(model_budget)

# Recent as reference
model_recent <- lm(rating ~ I(attribute == "importance_female") +
                            I(attribute == "importance_budget") +
                            I(attribute == "importance_political"),
                  data = regression_data)
recent_coef <- coef(model_recent)

# Create comparison matrix
comparison_matrix <- data.frame(
  Comparison = c("Female vs. Budget",
                 "Female vs. Recent",
                 "Female vs. Political",
                 "Budget vs. Recent",
                 "Budget vs. Political",
                 "Recent vs. Political"),
  Difference = c(-coef(ols_model)["is_budget"],
                 -coef(ols_model)["is_recent"],
                 -coef(ols_model)["is_political"],
                 budget_coef[3],
                 budget_coef[4],
                 recent_coef[4]),
  stringsAsFactors = FALSE
)

# Add confidence intervals and p-values from the models
comparison_matrix$Difference <- round(comparison_matrix$Difference, 3)

kable(comparison_matrix,
      caption = "All Pairwise Comparisons") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

## Equivalence Testing

Testing whether the importance of the female protagonist attribute is practically equivalent to other attributes.

```{r equivalence}
# Define equivalence bounds (e.g., ±0.5 on 7-point scale)
equiv_bound <- 0.5

# Get mean ratings for each attribute
mean_ratings <- clean_data %>%
  summarise(across(starts_with("importance_"), ~mean(., na.rm = TRUE)))

# Calculate differences from female protagonist rating
female_mean <- mean_ratings$importance_female
differences <- data.frame(
  Attribute = c("Big Budget", "Recent Release", "Political Leader"),
  Difference = c(
    mean_ratings$importance_budget - female_mean,
    mean_ratings$importance_recent - female_mean,
    mean_ratings$importance_political - female_mean
  )
) %>%
  mutate(
    Within_Bounds = abs(Difference) < equiv_bound,
    Interpretation = ifelse(Within_Bounds,
                           "Practically Equivalent",
                           "Meaningfully Different")
  )

kable(differences,
      digits = 2,
      caption = paste("Equivalence Testing (Bounds: ±", equiv_bound, ")", sep = "")) %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  row_spec(which(differences$Within_Bounds), bold = TRUE)
```

# Visualizations

## Importance Ratings Distribution

```{r rating-distribution}
# Create violin plot with box plot overlay
ggplot(regression_data, aes(x = attribute_label, y = rating, fill = attribute_label)) +
  geom_violin(alpha = 0.7, trim = FALSE) +
  geom_boxplot(width = 0.2, alpha = 0.9) +
  geom_jitter(width = 0.1, alpha = 0.3, size = 1) +
  stat_summary(fun = mean, geom = "point", shape = 23, size = 3,
               fill = "red", color = "red") +
  scale_y_continuous(breaks = 1:7, limits = c(0.5, 7.5)) +
  scale_fill_economist() +
  labs(title = "Distribution of Importance Ratings by Attribute",
       subtitle = "Gender Pretest Survey Results",
       x = "Film Attribute",
       y = "Importance Rating (1-7)",
       caption = "Red diamonds indicate mean values") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none")
```

## Mean Importance with Confidence Intervals

```{r mean-ci-plot}
# Calculate means and confidence intervals
ci_data <- regression_data %>%
  group_by(attribute_label) %>%
  summarise(
    mean = mean(rating, na.rm = TRUE),
    se = sd(rating, na.rm = TRUE) / sqrt(n()),
    lower = mean - 1.96 * se,
    upper = mean + 1.96 * se,
    .groups = "drop"
  ) %>%
  arrange(desc(mean))

# Create plot
ggplot(ci_data, aes(x = reorder(attribute_label, mean), y = mean)) +
  geom_point(size = 4, color = "darkblue") +
  geom_errorbar(aes(ymin = lower, ymax = upper),
                width = 0.2, size = 1, color = "darkblue") +
  geom_hline(yintercept = 4, linetype = "dashed", color = "gray50") +
  coord_flip() +
  scale_y_continuous(breaks = 1:7, limits = c(1, 7)) +
  labs(title = "Mean Importance Ratings with 95% Confidence Intervals",
       subtitle = "Gender Pretest Survey",
       x = "Film Attribute",
       y = "Mean Importance Rating",
       caption = "Dashed line indicates scale midpoint (4)") +
  theme_minimal() +
  theme(panel.grid.major.y = element_blank())
```

## Correlation Matrix

```{r correlation}
# Calculate correlation matrix
cor_matrix <- clean_data %>%
  select(starts_with("importance_")) %>%
  cor(use = "complete.obs")

# Rename for display
colnames(cor_matrix) <- rownames(cor_matrix) <-
  c("Female", "Budget", "Recent", "Political")

# Create correlation plot
library(corrplot)
corrplot(cor_matrix,
         method = "color",
         type = "upper",
         order = "hclust",
         addCoef.col = "black",
         tl.col = "black",
         tl.srt = 45,
         diag = FALSE,
         title = "Correlation Matrix of Importance Ratings",
         mar = c(0, 0, 2, 0))
```

# Key Findings

```{r key-findings}
# Identify key patterns
female_rank <- which(importance_summary$attribute == "Female Protagonist")
female_mean <- importance_summary$Mean[importance_summary$attribute == "Female Protagonist"]
overall_mean <- mean(importance_summary$Mean)

# Test if all means are within 1 point of each other
range_ratings <- max(importance_summary$Mean) - min(importance_summary$Mean)
all_similar <- range_ratings < 1

cat("========================================\n")
cat("HYPOTHESIS TEST RESULTS\n")
cat("========================================\n\n")
cat("Hypothesis: There should be NO significant differences in importance\n")
cat("           ratings across all four attributes\n\n")

cat("DESCRIPTIVE STATISTICS:\n")
cat("-----------------------\n")
cat("1. Sample size after exclusions: ", nrow(clean_data), "\n", sep = "")
cat("2. Female protagonist mean rating: ", female_mean, " (Rank: #", female_rank, "/4)\n", sep = "")
cat("3. Overall mean across all attributes: ", round(overall_mean, 2), "\n", sep = "")
cat("4. Range of mean ratings: ", round(range_ratings, 2), "\n", sep = "")
cat("5. All attributes within 1 point: ",
    ifelse(all_similar, "YES ✓", "NO ✗"), "\n\n", sep = "")

# Check OLS model significance
ols_p_value <- pf(summary(ols_model)$fstatistic[1],
                  summary(ols_model)$fstatistic[2],
                  summary(ols_model)$fstatistic[3],
                  lower.tail = FALSE)

cat("STATISTICAL SIGNIFICANCE TESTS:\n")
cat("-------------------------------\n")
cat("OLS Regression Results:\n")
cat("• Overall model F-statistic p-value: ", format(ols_p_value, scientific = FALSE, digits = 4), "\n", sep = "")

if(ols_p_value < 0.05) {
  cat("• Result: SIGNIFICANT differences detected (p < 0.05) ✗\n")
  cat("• Hypothesis REJECTED - There ARE significant differences\n\n")

  # Check which specific comparisons are significant
  sig_comparisons <- ols_summary %>%
    filter(significant == "Yes" & term != "Female Protagonist (Baseline)")
  if(nrow(sig_comparisons) > 0) {
    cat("Specific significant differences from Female Protagonist:\n")
    for(i in 1:nrow(sig_comparisons)) {
      cat("  - ", sig_comparisons$term[i], ": ",
          sprintf("%+.3f", sig_comparisons$estimate[i]),
          " (p = ", sprintf("%.3f", sig_comparisons$p.value[i]), ")\n", sep = "")
    }
  } else {
    cat("  No individual comparisons reached significance\n")
  }
} else {
  cat("• Result: NO significant differences detected (p ≥ 0.05) ✓\n")
  cat("• Hypothesis SUPPORTED - No significant differences in importance\n\n")

  # Show all comparison p-values even if not significant
  all_comparisons <- ols_summary %>%
    filter(term != "Female Protagonist (Baseline)")
  cat("All comparisons with Female Protagonist (for reference):\n")
  for(i in 1:nrow(all_comparisons)) {
    cat("  - ", all_comparisons$term[i], ": ",
        sprintf("%+.3f", all_comparisons$estimate[i]),
        " (p = ", sprintf("%.3f", all_comparisons$p.value[i]), ")\n", sep = "")
  }
}

cat("\n")
cat("CONCLUSION:\n")
cat("-----------\n")
if(ols_p_value >= 0.05) {
  cat("The data support the hypothesis that there are no significant\n")
  cat("differences in perceived importance across the four attributes.\n")
  cat("The female protagonist attribute is viewed as similarly important\n")
  cat("as the other control attributes (budget, recency, political leader).\n")
} else {
  cat("The data do not support the hypothesis. There are significant\n")
  cat("differences in perceived importance across the four attributes.\n")
  cat("Further investigation may be needed to determine if the female\n")
  cat("protagonist attribute should be adjusted or replaced.\n")
}

# Additional detail table
cat("\n")
cat("========================================\n")
cat("DETAILED MEAN COMPARISONS\n")
cat("========================================\n")

# Create comparison table
detail_table <- importance_summary %>%
  select(Attribute = attribute, Mean, SD, N) %>%
  mutate(
    `Difference from Female` = Mean - female_mean,
    `% Difference` = round((Mean - female_mean) / female_mean * 100, 1)
  )

print(kable(detail_table,
            digits = 2,
            caption = "Detailed Attribute Comparisons") %>%
        kable_styling(bootstrap_options = c("striped", "hover")))
```

# Session Information

```{r session-info}
sessionInfo()
```