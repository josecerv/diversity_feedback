---
title: "Gender Attribute Pretest Analysis - Biopic Film Attributes"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    theme: flatly
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
                      fig.width = 10, fig.height = 6)

# Load required libraries
library(tidyverse)
library(qualtRics)
library(kableExtra)
library(ggthemes)
library(broom)

# Set theme for all plots
theme_set(theme_economist())
```

# Data Collection

```{r api-setup}
# Set up Qualtrics API credentials
qualtrics_api_credentials(api_key = "BJ6aDiEgtQihneCgtZycVLYjaj0ao9gYkdRkb1UZ",
                          base_url = "yul1.qualtrics.com",
                          install = FALSE,
                          overwrite = TRUE)

# Survey ID for biopic gender pretest
survey_id <- "SV_2mky9SMX7AwaQya"

# List available surveys to verify connection
cat("Checking API connection...\n")
tryCatch({
  surveys <- all_surveys()
  cat("Successfully connected! Found", nrow(surveys), "surveys.\n")

  # Check if our survey exists
  if(survey_id %in% surveys$id) {
    cat("✓ Survey", survey_id, "found in account\n")
  } else {
    cat("✗ Survey", survey_id, "NOT found. Available survey IDs:\n")
    print(surveys$id)
  }
}, error = function(e) {
  cat("Error connecting to Qualtrics API:", conditionMessage(e), "\n")
})
```

```{r fetch-data}
# Fetch survey data using Qualtrics API
survey_data <- fetch_survey(surveyID = survey_id,
                            force_request = TRUE,
                            verbose = FALSE)

# Display initial data dimensions
cat("Initial sample size:", nrow(survey_data), "\n")
cat("Number of variables:", ncol(survey_data), "\n")
```

# Data Cleaning

```{r data-cleaning}
# Function to clean scale values - handles Qualtrics text anchors properly
clean_scale <- function(x) {
  x <- as.character(x)

  # Handle specific Qualtrics text anchors:
  x <- case_when(
    grepl("Not at all important.*1", x) ~ "1",
    grepl("Very important.*7", x) ~ "7",
    TRUE ~ x
  )

  # Remove any remaining non-numeric characters
  x <- gsub("[^0-9]", "", x)
  return(as.numeric(x))
}

# Clean and prepare data
clean_data <- survey_data %>%
  # Filter for completed responses
  filter(Finished == TRUE) %>%
  # Filter for those who passed attention checks
  filter(!is.na(ec_1_1) & !is.na(ec_2)) %>%
  # Select relevant columns
  select(ResponseId,
         # Importance ratings
         importance_female = importance_1,  # Feature a female protagonist
         importance_budget = importance_2,  # Had a big budget (>$40 million)
         importance_recent = importance_3,  # Were released after 2010
         importance_political = importance_4) %>% # Feature a political leader
  # Clean importance ratings (handle text anchors if present)
  mutate(across(starts_with("importance_"), clean_scale)) %>%
  # EXCLUDE participants who didn't answer ALL four importance questions
  filter(complete.cases(importance_female, importance_budget,
                        importance_recent, importance_political))

cat("Final sample size after cleaning:", nrow(clean_data), "\n")
```

# Statistical Analysis - Pairwise T-Tests

```{r t-tests}
# Function to calculate Cohen's d for paired samples
cohens_d_paired <- function(x, y) {
  diff <- x - y
  sd_diff <- sd(diff, na.rm = TRUE)
  mean_diff <- mean(diff, na.rm = TRUE)

  # Cohen's d for paired samples
  d <- mean_diff / sd_diff

  # Classify magnitude
  magnitude <- case_when(
    abs(d) < 0.2 ~ "negligible",
    abs(d) < 0.5 ~ "small",
    abs(d) < 0.8 ~ "medium",
    TRUE ~ "large"
  )

  return(list(estimate = d, magnitude = magnitude))
}

# Create list to store results
t_test_results <- list()

# Define attribute pairs for comparison
comparisons <- list(
  c("importance_female", "importance_budget", "Female Protagonist", "Big Budget"),
  c("importance_female", "importance_recent", "Female Protagonist", "Recent Release"),
  c("importance_female", "importance_political", "Female Protagonist", "Political Leader"),
  c("importance_budget", "importance_recent", "Big Budget", "Recent Release"),
  c("importance_budget", "importance_political", "Big Budget", "Political Leader"),
  c("importance_recent", "importance_political", "Recent Release", "Political Leader")
)

# Perform t-tests for each comparison
for(comp in comparisons) {
  var1 <- comp[1]
  var2 <- comp[2]
  label1 <- comp[3]
  label2 <- comp[4]

  # Paired t-test (since same participants rate all attributes)
  t_result <- t.test(clean_data[[var1]], clean_data[[var2]], paired = TRUE)

  # Calculate Cohen's d for effect size
  cohen_d <- cohens_d_paired(clean_data[[var1]], clean_data[[var2]])

  # Store results
  t_test_results[[paste(label1, "vs", label2)]] <- list(
    comparison = paste(label1, "vs", label2),
    mean_diff = mean(clean_data[[var1]] - clean_data[[var2]], na.rm = TRUE),
    t_stat = t_result$statistic,
    df = t_result$parameter,
    p_value = t_result$p.value,
    ci_lower = t_result$conf.int[1],
    ci_upper = t_result$conf.int[2],
    cohens_d = cohen_d$estimate,
    d_magnitude = cohen_d$magnitude
  )
}

# Convert to data frame
t_test_df <- bind_rows(t_test_results) %>%
  mutate(
    # Apply multiple comparison corrections
    p_bonferroni = p.adjust(p_value, method = "bonferroni", n = 6),
    p_holm = p.adjust(p_value, method = "holm", n = 6),

    # Determine significance at different correction levels
    sig_uncorrected = ifelse(p_value < 0.05, "Yes", "No"),
    sig_bonferroni = ifelse(p_bonferroni < 0.05, "Yes", "No"),
    sig_holm = ifelse(p_holm < 0.05, "Yes", "No"),

    # Format for display
    mean_diff = round(mean_diff, 3),
    t_stat = round(t_stat, 2),
    cohens_d = round(cohens_d, 3),
    p_value = round(p_value, 4),
    p_bonferroni = round(p_bonferroni, 4),
    p_holm = round(p_holm, 4),
    ci_lower = round(ci_lower, 3),
    ci_upper = round(ci_upper, 3)
  )

# Display main results table
kable(t_test_df %>%
        select(Comparison = comparison,
               `Mean Diff` = mean_diff,
               `t` = t_stat,
               `df` = df,
               `p-value` = p_value,
               `p (Bonf.)` = p_bonferroni,
               `Cohen's d` = cohens_d,
               `Effect Size` = d_magnitude),
      caption = "Pairwise T-Tests Between Film Attributes") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  row_spec(which(t_test_df$sig_uncorrected == "Yes"), bold = TRUE) %>%
  footnote(general = "Bold rows indicate p < 0.05 (uncorrected). Bonferroni correction applied for 6 comparisons.")

# Summary of significant findings
cat("\n=== T-TEST SUMMARY ===\n")
cat("Total comparisons:", nrow(t_test_df), "\n")
cat("Significant (uncorrected):", sum(t_test_df$sig_uncorrected == "Yes"), "\n")
cat("Significant (Holm correction):", sum(t_test_df$sig_holm == "Yes"), "\n")
cat("Significant (Bonferroni):", sum(t_test_df$sig_bonferroni == "Yes"), "\n\n")

# Check if any comparison involving female protagonist is significant
female_comparisons <- t_test_df %>%
  filter(grepl("Female Protagonist", comparison))

if(any(female_comparisons$sig_bonferroni == "Yes")) {
  cat("WARNING: Significant differences found with Female Protagonist attribute\n")
  cat("even after Bonferroni correction:\n")
  sig_female <- female_comparisons %>%
    filter(sig_bonferroni == "Yes")
  for(i in 1:nrow(sig_female)) {
    cat("  -", sig_female$comparison[i], "(p =", sig_female$p_bonferroni[i], ")\n")
  }
} else {
  cat("✓ No significant differences with Female Protagonist attribute\n")
  cat("  after correcting for multiple comparisons\n")
}
```

# Key Findings

```{r key-findings}
cat("========================================\n")
cat("HYPOTHESIS TEST RESULTS\n")
cat("========================================\n\n")
cat("Hypothesis: There should be NO significant differences in importance\n")
cat("           ratings across all four film attributes\n\n")

# Calculate summary stats
importance_summary <- clean_data %>%
  select(starts_with("importance_")) %>%
  pivot_longer(everything(), names_to = "attribute", values_to = "rating") %>%
  mutate(attribute = case_when(
    attribute == "importance_female" ~ "Female Protagonist",
    attribute == "importance_budget" ~ "Big Budget (>$40M)",
    attribute == "importance_recent" ~ "Released After 2010",
    attribute == "importance_political" ~ "Political Leader"
  )) %>%
  group_by(attribute) %>%
  summarise(
    N = n(),
    Mean = round(mean(rating, na.rm = TRUE), 2),
    SD = round(sd(rating, na.rm = TRUE), 2),
    .groups = "drop"
  ) %>%
  arrange(desc(Mean))

# Print summary
kable(importance_summary,
      caption = "Mean Importance Ratings by Attribute") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

# Identify key patterns
female_mean <- importance_summary$Mean[importance_summary$attribute == "Female Protagonist"]
overall_mean <- mean(importance_summary$Mean)
range_ratings <- max(importance_summary$Mean) - min(importance_summary$Mean)

cat("\nDESCRIPTIVE STATISTICS:\n")
cat("-----------------------\n")
cat("1. Sample size after exclusions: ", nrow(clean_data), "\n", sep = "")
cat("2. Female protagonist mean rating: ", female_mean, "\n", sep = "")
cat("3. Overall mean across all attributes: ", round(overall_mean, 2), "\n", sep = "")
cat("4. Range of mean ratings: ", round(range_ratings, 2), "\n\n", sep = "")

cat("CONCLUSION:\n")
cat("-----------\n")
if(all(female_comparisons$sig_bonferroni == "No")) {
  cat("The data support the hypothesis that there are no significant\n")
  cat("differences in perceived importance across the four film attributes.\n")
  cat("The female protagonist attribute is viewed as similarly important\n")
  cat("as the other control attributes (budget, recency, political leader).\n")
} else {
  cat("The data do not support the hypothesis. There are significant\n")
  cat("differences in perceived importance across the four film attributes.\n")
  cat("Further investigation may be needed to determine if the female\n")
  cat("protagonist attribute should be adjusted or replaced.\n")
}
```

# Session Information

```{r session-info}
sessionInfo()
```